{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://warwick.ac.uk/fac/cross_fac/tia/data/glascontest/about/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbIWX3-ioEnA",
    "outputId": "26f669a9-e6aa-417c-d8ac-b7ecb659a415"
   },
   "outputs": [],
   "source": [
    "# # download file from https://warwick.ac.uk/fac/cross_fac/tia/data/glascontest/download\n",
    "!wget https://warwick.ac.uk/fac/cross_fac/tia/data/glascontest/download/warwick_qu_dataset_released_2016_07_08.zip\n",
    "# extract zip-file\n",
    "!unzip -qq \"warwick_qu_dataset_released_2016_07_08.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZPQtpGH1jcW",
    "outputId": "c32e11bc-8174-4c84-f5ae-b352b61f5c91"
   },
   "outputs": [],
   "source": [
    "!ls \"Warwick QU Dataset (Released 2016_07_08)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfEKmePzqscI"
   },
   "outputs": [],
   "source": [
    "import os # to use makedirs for create directory\n",
    "import shutil # copy, move file or directory \n",
    "from glob import glob # grap expected files\n",
    "from natsort import natsorted # natural sorting\n",
    "\n",
    "# mask\n",
    "train_mask = natsorted(glob('Warwick QU Dataset (Released 2016_07_08)/train*_anno.bmp'))\n",
    "path = 'Warwick QU Dataset (Released 2016_07_08)/train/mask/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for t in train_mask:\n",
    "    src = t\n",
    "    dst = f'{path}{t.split(\"/\")[-1]}'\n",
    "    shutil.move(src,dst)\n",
    "\n",
    "# image\n",
    "train_image = natsorted(glob('Warwick QU Dataset (Released 2016_07_08)/train*.bmp'))\n",
    "path = 'Warwick QU Dataset (Released 2016_07_08)/train/image/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for t in train_image:\n",
    "    src = t\n",
    "    dst = f'{path}{t.split(\"/\")[-1]}'\n",
    "    shutil.move(src,dst)\n",
    "\n",
    "# mask\n",
    "test_mask = natsorted(glob('Warwick QU Dataset (Released 2016_07_08)/test*_anno.bmp'))\n",
    "path = 'Warwick QU Dataset (Released 2016_07_08)/test/mask/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for t in test_mask:\n",
    "    src = t\n",
    "    dst = f'{path}{t.split(\"/\")[-1]}'\n",
    "    shutil.move(src,dst)\n",
    "\n",
    "# image\n",
    "test_image = natsorted(glob('Warwick QU Dataset (Released 2016_07_08)/test*.bmp'))\n",
    "path = 'Warwick QU Dataset (Released 2016_07_08)/test/image/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for t in test_image:\n",
    "    src = t\n",
    "    dst = f'{path}{t.split(\"/\")[-1]}'\n",
    "    shutil.move(src,dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJ685xTp2ITV"
   },
   "outputs": [],
   "source": [
    "# build validation dataset from train dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# mask\n",
    "train_mask = natsorted(glob('Warwick QU Dataset (Released 2016_07_08)/train/mask/*.bmp'))\n",
    "_,valid_mask = train_test_split(train_mask,test_size=0.1,shuffle=True,random_state=0)\n",
    "path = 'Warwick QU Dataset (Released 2016_07_08)/valid/mask/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for t in valid_mask:\n",
    "    src = t\n",
    "    dst = f'{path}{t.split(\"/\")[-1]}'\n",
    "    shutil.move(src,dst)\n",
    "\n",
    "# image\n",
    "train_image = natsorted(glob('Warwick QU Dataset (Released 2016_07_08)/train/image/*.bmp'))\n",
    "_,valid_image = train_test_split(train_image,test_size=0.1,shuffle=True,random_state=0)\n",
    "path = 'Warwick QU Dataset (Released 2016_07_08)/valid/image/'\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for t in valid_image:\n",
    "    src = t\n",
    "    dst = f'{path}{t.split(\"/\")[-1]}'\n",
    "    shutil.move(src,dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pb891zMOoG1F"
   },
   "outputs": [],
   "source": [
    "!pip install livelossplot torchmetrics monai --quiet\n",
    "\n",
    "import random,cv2\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torch.utils.data\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import patches\n",
    "\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pv3Ch1f-pUgs",
    "outputId": "71a89723-1cf6-45fc-a004-7a6a844abda3"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"The code will run on GPU.\")\n",
    "else:\n",
    "    print(\"The code will run on CPU. Go to Edit->Notebook Settings and choose GPU as the hardware accelerator\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXV3O1LL2DdK"
   },
   "outputs": [],
   "source": [
    "image = Image.open('Warwick QU Dataset (Released 2016_07_08)/train/image/train_1.bmp')\n",
    "mask = Image.open('Warwick QU Dataset (Released 2016_07_08)/train/mask/train_1_anno.bmp')\n",
    "plt.subplot(121)\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "plt.subplot(122)\n",
    "plt.axis('off')\n",
    "plt.imshow(mask)\n",
    "np.unique(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pmrxDEWtoUsI",
    "outputId": "6fa0b107-ad61-46d9-906c-32f4b5d8c7da"
   },
   "outputs": [],
   "source": [
    "class WarwickCellDataset(object):\n",
    "    def __init__(self, root, transforms=None): # transforms\n",
    "        self.root = root\n",
    "        self.transforms=[]\n",
    "        if transforms!=None:\n",
    "            self.transforms.append(transforms)\n",
    "            \n",
    "        # load all image files, sorting them to ensure that they are aligned\n",
    "        self.imgs = list(natsorted(os.listdir(os.path.join(root, \"image\"))))\n",
    "        self.masks = list(natsorted(os.listdir(os.path.join(root, \"mask\"))))\n",
    "        print('imgs file names:\\n', self.imgs)\n",
    "        print('masks file names:\\n', self.masks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"image\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"mask\", self.masks[idx])\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path) # with 0 being background, 1 ~ N are for different instance\n",
    "        mask = np.array(mask)        # convert the PIL Image into a numpy array\n",
    "        obj_ids = np.unique(mask)    # instances are encoded as different colors\n",
    "        obj_ids = obj_ids[1:]        # first id is the background, so remove it\n",
    "\n",
    "        # split the color-encoded mask into a set of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            A = abs((xmax-xmin) * (ymax-ymin)) \n",
    "            \n",
    "            # Check if area is larger than a threshold (here 5)\n",
    "            if A < 5:\n",
    "                obj_ids=np.delete(obj_ids, [i])\n",
    "                continue\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)     # convert everything into a torch.Tensor\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)     # there is only one class\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        # area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        masks_original = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "        masks_original[masks_original!=0] = 1\n",
    "        \n",
    "        for i in self.transforms:\n",
    "            img = i(img)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"masks_original\"] = masks_original\n",
    "        return img.double(), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "root_train = 'Warwick QU Dataset (Released 2016_07_08)/train'\n",
    "root_valid = 'Warwick QU Dataset (Released 2016_07_08)/valid'\n",
    "root_test = 'Warwick QU Dataset (Released 2016_07_08)/test'\n",
    "dataset_train = WarwickCellDataset(root_train, transforms=torchvision.transforms.ToTensor()) # get_transform(train=True)\n",
    "dataset_valid = WarwickCellDataset(root_valid, transforms=torchvision.transforms.ToTensor())\n",
    "dataset_test = WarwickCellDataset(root_test, transforms=torchvision.transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EZ_peVr_qInW",
    "outputId": "2fb15d15-bfe2-4217-a179-19b445d711da"
   },
   "outputs": [],
   "source": [
    "# Define data loader\n",
    "data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=2, num_workers=2, shuffle=True, collate_fn=lambda x:list(zip(*x)))\n",
    "data_loader_valid = torch.utils.data.DataLoader(dataset_valid, batch_size=2, num_workers=1, shuffle=False, collate_fn=lambda x:list(zip(*x)))\n",
    "data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=lambda x:list(zip(*x)))\n",
    "print(len(data_loader_train),len(data_loader_valid),len(data_loader_test))\n",
    "\n",
    "# collate_fn=lambda x:list(zip(*x)) is for variant ROIs\n",
    "# data_loader_train = torch.utils.data.DataLoader(dataset_train, batch_size=2, num_workers=2, shuffle=True) # <-- this will have error when we run following steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "dRHPIvJ9s7vP",
    "outputId": "5e4530cb-37f0-4a30-d37a-d24823c38c3f"
   },
   "outputs": [],
   "source": [
    "# View some images and their bbox's in the training set.\n",
    "images,labels=next(iter(data_loader_train))\n",
    "\n",
    "def random_colour_masks(image):\n",
    "    \"\"\"\n",
    "    for visualizing instance segmentation\n",
    "    \"\"\"\n",
    "    colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "    r[image == 1], g[image == 1], b[image == 1] = colours[random.randrange(0,10)]\n",
    "    coloured_mask = np.stack([r, g, b], axis=-1)\n",
    "    return coloured_mask\n",
    "\n",
    "def apply_nms(labels, boxes, masks, scores, iou_thresh=0.5):\n",
    "    \"\"\"\n",
    "    Non-maximum suppression (NMS) using torchvision\n",
    "    it works on a single target\n",
    "    \"\"\"\n",
    "    keep = torchvision.ops.nms(boxes, scores, iou_thresh)  # torchvision returns the indices of the bboxes to keep\n",
    "    \n",
    "    labels = labels[keep]\n",
    "    boxes = boxes[keep]\n",
    "    masks = masks[keep]\n",
    "    scores = scores[keep]\n",
    "    return labels, boxes, masks, scores\n",
    "\n",
    "def view(images, annos, score_threshold=0.5, nms_iou_threshold = 0.5, n=2, std=1, mean=0):\n",
    "    \"\"\"\n",
    "    view image, label, result for object detection\n",
    "    score_threshold : remove boxes when confidence score is lower than the value\n",
    "    nms_iou_threshold : remove duplicated boxes using NMS\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(15,10))\n",
    "    images=list(images)\n",
    "    annos=list(annos)\n",
    "    \n",
    "    for i in range(n):\n",
    "        image=images[i].cpu().numpy().transpose((1,2,0)) # torch type to numpy type to show image using plt\n",
    "        image=np.array(std)*image+np.array(mean)\n",
    "        image=np.clip(image,0,1) * 255\n",
    "        image=image.astype(np.uint8) \n",
    "\n",
    "        l=annos[i]['labels']\n",
    "        b=annos[i]['boxes']\n",
    "        m=annos[i]['masks']\n",
    "        try:\n",
    "            scores=annos[i]['scores']\n",
    "            if nms_iou_threshold:\n",
    "                l,b,m,scores = apply_nms(l, b, m, scores, nms_iou_threshold)\n",
    "        except:\n",
    "            scores = [\"GT\"]*len(b)\n",
    "        \n",
    "        if len(m.shape)==4:\n",
    "            m=m[:,0]\n",
    "        \n",
    "        l = l.cpu().numpy()\n",
    "        b = b.cpu().numpy()\n",
    "        m = m.cpu().numpy()\n",
    "        \n",
    "        # mask likelihood to binary[0,1] \n",
    "        m[m>=.5]=1\n",
    "        m[m<.5]=0\n",
    "         \n",
    "        # (X1,Y1,X2,Y2) to (X1,Y1,W,H)\n",
    "        b[:,2]=b[:,2]-b[:,0]\n",
    "        b[:,3]=b[:,3]-b[:,1]\n",
    "        \n",
    "        ax = figure.add_subplot(2,2, i + 1)\n",
    "        for j in range(len(b)):\n",
    "            if scores[j]=='GT' or scores[j]>score_threshold:\n",
    "                ax.add_patch(patches.Rectangle((b[j][0],b[j][1]),b[j][2],b[j][3],linewidth=1.5,edgecolor='g',facecolor='none'))\n",
    "                try:\n",
    "                    ax.text(int(b[j][0]), int(b[j][1]), f\"{scores[j]:.3f}\", fontsize=10)\n",
    "                except:\n",
    "                    pass\n",
    "                mask = random_colour_masks(m[j])\n",
    "                image = cv2.addWeighted(image, 1, mask, 0.4, 0)\n",
    "        ax.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "view(images=images,annos=labels, score_threshold=0, nms_iou_threshold=0, n=2, std=1, mean=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "ratios = []\n",
    "for idx,batch in enumerate(data_loader_train):\n",
    "    img, target = batch\n",
    "    # print(target)\n",
    "    boxes = target[0]['boxes']\n",
    "    for b in boxes:\n",
    "        x_delta = b[2]-b[0]\n",
    "        y_delta = b[3]-b[1]\n",
    "        xs.append(x_delta)\n",
    "        ys.append(y_delta)\n",
    "        ratios.append(y_delta/x_delta)        \n",
    "print('train dataset')\n",
    "print(f'xs min {np.min(xs)}, max {np.max(xs)}, mean {np.mean(xs)}, median {np.median(xs)}')\n",
    "print(f'ys min {np.min(ys)}, max {np.max(ys)}, mean {np.mean(ys)}, median {np.median(ys)}')\n",
    "print(f'ratios min {np.min(ratios)}, max {np.max(ratios)}, mean {np.mean(ratios)}, median {np.median(ratios)}')\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "ratios = []\n",
    "for idx,batch in enumerate(data_loader_valid):\n",
    "    img, target = batch\n",
    "    # print(target)\n",
    "    boxes = target[0]['boxes']\n",
    "    for b in boxes:\n",
    "        x_delta = b[2]-b[0]\n",
    "        y_delta = b[3]-b[1]\n",
    "        xs.append(x_delta)\n",
    "        ys.append(y_delta)\n",
    "        ratios.append(y_delta/x_delta)\n",
    "print('valid dataset')\n",
    "print(f'xs min {np.min(xs)}, max {np.max(xs)}, mean {np.mean(xs)}, median {np.median(xs)}')\n",
    "print(f'ys min {np.min(ys)}, max {np.max(ys)}, mean {np.mean(ys)}, median {np.median(ys)}')\n",
    "print(f'ratios min {np.min(ratios)}, max {np.max(ratios)}, mean {np.mean(ratios)}, median {np.median(ratios)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Mask R-CNN, finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, RPNHead\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "\n",
    "# load an instance segmentation model pre-trained on COCO\n",
    "# model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\") # torch >= 1.11 \n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True) # torch < 1.11\n",
    "# model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False) # torch < 1.11\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify model for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCd59JGYoaMi"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features    # get number of input features for the classifier\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) # replace the pre-trained head with a new one\n",
    "\n",
    "in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels # now get the number of input features for the mask classifier\n",
    "hidden_layer = 256\n",
    "model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes) # and replace the mask predictor with a new one\n",
    "\n",
    "anchor_sizes = ((32,), (64,), (128,), (256,), (512,),)\n",
    "anchor_generator = AnchorGenerator(sizes = anchor_sizes, aspect_ratios = ((.125, .25, 0.5, 1.0, 2.0, 4.0, 8.0),) * len(anchor_sizes))\n",
    "model.rpn.anchor_generator = anchor_generator\n",
    "\n",
    "rpn_head = RPNHead(model.backbone.out_channels, anchor_generator.num_anchors_per_location()[0])\n",
    "model.rpn.head = rpn_head\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model=model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.002, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "lr_scheduler.get_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "BacmoAOvklfZ",
    "outputId": "e755c46f-02af-4bd2-a94d-f594b76969cb"
   },
   "outputs": [],
   "source": [
    "# check the performance of pretrained weight\n",
    "images, targets=next(iter(data_loader_test))\n",
    "images = list(image.to(device) for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "model=model.double()\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    view(images=images,annos=outputs, score_threshold=0, nms_iou_threshold=0, n=1, std=1, mean=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 117,
     "referenced_widgets": [
      "c454ee989ce048c5973981326ac981d7"
     ]
    },
    "id": "ZAp10VJ1olMH",
    "outputId": "6691412b-4ffd-45bd-eeb2-8f3161891cd3"
   },
   "outputs": [],
   "source": [
    "from livelossplot import PlotLosses\n",
    "plotlosses = PlotLosses()\n",
    "\n",
    "# Perform training loop for n epochs\n",
    "loss_train_epoch = []\n",
    "loss_valid_epoch = []\n",
    "n_epochs = 20\n",
    "\n",
    "for epoch in tqdm(range(n_epochs),desc='training'):\n",
    "    model.train()\n",
    "    loss_train = []\n",
    "    for images,targets in tqdm(data_loader_train,desc=f'train_epoch_{epoch}'):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        optimizer.zero_grad()\n",
    "        model=model.double()\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()       \n",
    "        optimizer.step()        \n",
    "        loss_train.append(losses.item())\n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    # model.eval()\n",
    "    loss_valid = []\n",
    "    for images,targets in tqdm(data_loader_valid,desc=f'valid_epoch_{epoch}'):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        model=model.double()\n",
    "        with torch.no_grad():\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            loss_valid.append(losses.item())\n",
    "    \n",
    "    loss_train_mean = np.mean(loss_train) \n",
    "    loss_train_epoch.append(loss_train_mean)\n",
    "\n",
    "    loss_valid_mean = np.mean(loss_valid) \n",
    "    loss_valid_epoch.append(loss_valid_mean)\n",
    "\n",
    "    plotlosses.update({\n",
    "    'loss': loss_train_mean,\n",
    "    'val_loss': loss_valid_mean,\n",
    "    'lr': lr_scheduler.get_lr()[0],\n",
    "    })\n",
    "    plotlosses.send()\n",
    "\n",
    "    # Save model\n",
    "    if loss_valid_epoch[-1] == np.min(loss_valid_epoch):\n",
    "        save_path = 'weight.pt'\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('weight.pt') # load fine-tuned state_dict\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.to(device)\n",
    "\n",
    "# if you have finetuned weight\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# state_dict = torch.load('drive/MyDrive/0.Dataset_public/weight.pt') # load fine-tuned state_dict\n",
    "# model.load_state_dict(state_dict)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize result (NMS, score thresholding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "YGY-MOKptzaU",
    "outputId": "98d40770-9964-44d3-8273-1855b7f7b014"
   },
   "outputs": [],
   "source": [
    "# Look at some of the images and predicted bbox's after training\n",
    "images, targets=next(iter(data_loader_test))\n",
    "images = list(image.to(device) for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "model=model.double()\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    view(images=images,annos=targets, score_threshold=0, nms_iou_threshold=0, n=1, std=1, mean=0)\n",
    "    view(images=images,annos=outputs, score_threshold=0.4, nms_iou_threshold=0.5, n=1, std=1, mean=0)\n",
    "    view(images=images,annos=outputs, score_threshold=0.4, nms_iou_threshold=0.3, n=1, std=1, mean=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,batch in enumerate(data_loader_test):\n",
    "    images, targets=batch \n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    model=model.double()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        view(images=images,annos=targets, score_threshold=0.7, nms_iou_threshold=0.7, n=1, std=1, mean=0)\n",
    "        view(images=images,annos=outputs, score_threshold=0.7, nms_iou_threshold=0.7, n=1, std=1, mean=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Mean Average Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YC0aGfRqt9Gk"
   },
   "outputs": [],
   "source": [
    "from torchmetrics.detection.map import MeanAveragePrecision\n",
    "# from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "metric = MeanAveragePrecision()\n",
    "\n",
    "for idx,batch in enumerate(data_loader_test):\n",
    "    images, targets=batch \n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    model=model.double()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        metric.update(outputs, targets)\n",
    "        \n",
    "from pprint import pprint\n",
    "pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (FROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "\n",
    "fp_probs = []\n",
    "tp_probs = []\n",
    "num_targets = []\n",
    "\n",
    "for idx, batch in enumerate(data_loader_test):\n",
    "    images, targets = batch \n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    model=model.double()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        \n",
    "        prob = outputs[0]['scores'].cpu().detach().numpy()\n",
    "        x = ((outputs[0]['boxes'][:,0]+outputs[0]['boxes'][:,2])/2).cpu().detach().numpy()#.astype(np.uint8)\n",
    "        y = ((outputs[0]['boxes'][:,1]+outputs[0]['boxes'][:,3])/2).cpu().detach().numpy()#.astype(np.uint8)\n",
    "        m = targets[0]['masks_original'].cpu().detach().numpy()\n",
    "        m = m.astype(np.uint8)\n",
    "        \n",
    "        fp_prob,tp_prob,num_target = monai.metrics.compute_fp_tp_probs(prob, y, x, m, labels_to_exclude=None, resolution_level=0)\n",
    "        fp_probs.extend(fp_prob)\n",
    "        tp_probs.extend(tp_prob)\n",
    "        num_targets.append(num_target)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_, y_ = monai.metrics.compute_froc_curve_data(fp_probs,tp_probs,len(num_targets),len(num_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('FROC')\n",
    "plt.plot(x_, y_)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
